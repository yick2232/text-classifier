{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BiLSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yick/Projects/github.com/text-classifier\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### build model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "\n",
    "WORD_VECTOR_PATH = \"/home/yick/Models/tencent/embeddings/light_Tencent_AILab_ChineseEmbedding.txt\"\n",
    "\n",
    "def load_wv(vocab, fpath=WORD_VECTOR_PATH):\n",
    "    word2vec = {}\n",
    "    embedding_dim = None\n",
    "    with open(fpath) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            if i == 0:\n",
    "                embedding_dim = int(values[1])\n",
    "                continue\n",
    "            if len(values) != embedding_dim + 1:\n",
    "                print(f\"error values: {values[:5]}, values len: {len(values)}\")\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word2vec[word] = coefs\n",
    "    print(f\"Found {len(word2vec)} word vectors.\" )\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_mat = np.random.rand(vocab_size+1, embedding_dim)\n",
    "    word_embedding_cnt = 0\n",
    "    for word, i in vocab.items():\n",
    "        if word in word2vec:\n",
    "            word_embedding_cnt += 1\n",
    "            embedding_mat[i] = word2vec.get(word)\n",
    "    print(f\"vocab size: {vocab_size}\")\n",
    "    print(f\"word_embedding_cnt: {word_embedding_cnt}\")\n",
    "    return embedding_mat, embedding_dim\n",
    "\n",
    "\n",
    "def build_model(vocab, num_classes, max_len=30):\n",
    "    embedding_mat, embedding_dim = load_wv(vocab)\n",
    "\n",
    "    inputs = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    embeddings = Embedding(\n",
    "        input_dim=len(vocab)+1,\n",
    "\t\toutput_dim=embedding_dim,\n",
    "\t\tinput_length=max_len,\n",
    "\t\tweights=[embedding_mat],\n",
    "\t\t# embeddings_regularizer=l2(0.00),\n",
    "\t\ttrainable=True\n",
    "    )(inputs)\n",
    "    print(f\"embeddings: {embeddings.shape}\")\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(embeddings)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(\n",
    "        num_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_regularizer=l2(1e-4)\n",
    "    )(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "\t\toptimizer=Adam(lr=5e-3),\n",
    "\t\tmetrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (8718, 2)\n",
      "test_df shape: (741, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "print(f\"train_df shape: {train_df.shape}\")\n",
    "print(f\"test_df shape: {test_df.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make label encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (8718, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(train_df[\"label\"].tolist())\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y = to_categorical(labels, num_classes=num_classes)\n",
    "print(f\"y shape: {y.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2392\n",
      "X shape: (8718, 30)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "texts = [[w for w in jieba.cut(d)] for d in train_df[\"text\"].tolist()]\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "vocab = tokenizer.word_index\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"X shape: {X.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error values: ['中共中央', '国务院关于完善产权保护制度依法保护产权的意见', '0.141571', '-0.006408', '-0.813869'], values len: 202\n",
      "error values: ['杨', '光', '-0.045217', '-0.197674', '0.007343'], values len: 202\n",
      "error values: ['王', '琪', '0.052169', '-0.368297', '-0.304854'], values len: 202\n",
      "error values: ['我', '末代工农兵学员', '0.485441', '0.84239', '0.347323'], values len: 202\n",
      "error values: ['财政部', '国家税务总局关于非货币性资产投资企业所得税政策问题的通知', '-0.088186', '-0.23139', '0.024681'], values len: 202\n",
      "Found 143607 word vectors.\n",
      "vocab size: 2392\n",
      "word_embedding_cnt: 2034\n",
      "embeddings: (?, 30, 200)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 200)           478600    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 30, 256)           336896    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 54)                6966      \n",
      "=================================================================\n",
      "Total params: 872,894\n",
      "Trainable params: 872,382\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Train on 8282 samples, validate on 436 samples\n",
      "Epoch 1/20\n",
      " - 12s - loss: 2.3795 - acc: 0.4455 - val_loss: 1.5147 - val_acc: 0.6193\n",
      "Epoch 2/20\n",
      " - 10s - loss: 1.1376 - acc: 0.7176 - val_loss: 1.0661 - val_acc: 0.7408\n",
      "Epoch 3/20\n",
      " - 10s - loss: 0.8434 - acc: 0.7973 - val_loss: 0.9036 - val_acc: 0.8050\n",
      "Epoch 4/20\n",
      " - 10s - loss: 0.6531 - acc: 0.8376 - val_loss: 1.2238 - val_acc: 0.7225\n",
      "Epoch 5/20\n",
      " - 10s - loss: 0.5821 - acc: 0.8545 - val_loss: 1.5716 - val_acc: 0.7133\n",
      "Epoch 6/20\n",
      " - 10s - loss: 0.5139 - acc: 0.8704 - val_loss: 0.8145 - val_acc: 0.8303\n",
      "Epoch 7/20\n",
      " - 10s - loss: 0.4861 - acc: 0.8765 - val_loss: 0.9440 - val_acc: 0.7867\n",
      "Epoch 8/20\n",
      " - 10s - loss: 0.4830 - acc: 0.8807 - val_loss: 2.3333 - val_acc: 0.5482\n",
      "Epoch 9/20\n",
      " - 10s - loss: 0.4471 - acc: 0.8939 - val_loss: 0.7037 - val_acc: 0.8555\n",
      "Epoch 10/20\n",
      " - 10s - loss: 0.4482 - acc: 0.8933 - val_loss: 1.1904 - val_acc: 0.7523\n",
      "Epoch 11/20\n",
      " - 11s - loss: 0.4034 - acc: 0.9086 - val_loss: 0.8931 - val_acc: 0.8326\n",
      "Epoch 12/20\n",
      " - 11s - loss: 0.4031 - acc: 0.9071 - val_loss: 1.7070 - val_acc: 0.6720\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fd211386a10>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = build_model(vocab, num_classes, max_len=30)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    mode=\"min\"\n",
    ")\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.05,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "              上征信     0.0526    1.0000    0.1000         1\n",
      "            之前被拒了     0.0000    0.0000    0.0000         1\n",
      "             人工服务     1.0000    0.4286    0.6000         7\n",
      "             什么平台     0.9615    0.8065    0.8772        31\n",
      "          会不会放款失败     0.0000    0.0000    0.0000         4\n",
      "              利息高     0.2857    0.3333    0.3077         6\n",
      "加下微信/发信息/发个短信/发资料     0.0000    0.0000    0.0000         0\n",
      "           号码是哪来的     0.0000    0.0000    0.0000         0\n",
      "            否定/拒绝     0.9394    0.3690    0.5299        84\n",
      "          咨询APP名字     0.6250    1.0000    0.7692         5\n",
      "          咨询利息/费用     0.8667    0.7027    0.7761        37\n",
      "           咨询提前还款     1.0000    1.0000    1.0000         1\n",
      "             咨询操作     1.0000    0.2333    0.3784        30\n",
      "           咨询放款速度     0.2308    0.7500    0.3529         4\n",
      "          咨询额度-通用     0.5488    0.8333    0.6618        54\n",
      "        嗯啊哦额/模糊回答     0.5000    0.4444    0.4706         9\n",
      "       在忙/在开会/在开车     0.7143    0.5000    0.5882        10\n",
      "           已经申请到了     0.0000    0.0000    0.0000         1\n",
      "             征信不好     0.0000    0.0000    0.0000         1\n",
      "             快说重点     0.0000    0.0000    0.0000         0\n",
      "             怎么提额     0.0000    0.0000    0.0000         0\n",
      "         怎么登录APP？     0.0000    0.0000    0.0000         0\n",
      "              恢复词     0.2889    0.5200    0.3714        25\n",
      "             打款渠道     0.0000    0.0000    0.0000         1\n",
      "        打过了/不要打电话     0.0000    0.0000    0.0000         6\n",
      "              打错了     0.1000    1.0000    0.1818         2\n",
      "               投诉     0.5714    0.7619    0.6531        21\n",
      "              提额券     0.0000    0.0000    0.0000         0\n",
      "             操作繁琐     0.0000    0.0000    0.0000         0\n",
      "          无法登陆APP     0.2857    1.0000    0.4444         2\n",
      "   有信用问题/逾期记录能借款吗     0.0000    0.0000    0.0000         4\n",
      "              有印象     0.0000    0.0000    0.0000         0\n",
      "               期数     0.0000    0.0000    0.0000         2\n",
      "             标记号码     0.0000    0.0000    0.0000         0\n",
      "              没印象     0.0000    0.0000    0.0000         0\n",
      "           没通过怎么办     1.0000    0.3000    0.4615        30\n",
      "          犹豫中/考虑下     0.0000    0.0000    0.0000         8\n",
      "             申请条件     0.0000    0.0000    0.0000         8\n",
      "             电话号码     0.5000    0.2500    0.3333         4\n",
      "               肯定     0.9920    0.5124    0.6757       242\n",
      "             调戏AI     0.0000    0.0000    0.0000         0\n",
      "            质疑机器人     0.5714    1.0000    0.7273         4\n",
      "           运营商提示音     1.0000    0.6591    0.7945        44\n",
      "              通过率     0.0000    0.0000    0.0000         2\n",
      "               重复     0.7857    0.5789    0.6667        19\n",
      "             额度太少     0.1500    0.5455    0.2353        11\n",
      "             额度循环     0.0000    0.0000    0.0000         1\n",
      "             额度用途     0.0000    0.0000    0.0000         1\n",
      "            骗子/骗人     0.8889    0.4444    0.5926        18\n",
      "\n",
      "         accuracy                         0.5169       741\n",
      "        macro avg     0.3236    0.3260    0.2765       741\n",
      "     weighted avg     0.8054    0.5169    0.5895       741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_texts = [[w for w in jieba.cut(d)] for d in test_df[\"text\"].tolist()]\n",
    "test_x = tokenizer.texts_to_sequences(test_texts)\n",
    "test_x = pad_sequences(test_x, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "probs = model.predict(test_x)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(preds.tolist())\n",
    "true_labels = test_df[\"label\"].tolist()\n",
    "report = classification_report(true_labels, pred_labels, digits=4)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-1.3",
   "language": "python",
   "display_name": "tf-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
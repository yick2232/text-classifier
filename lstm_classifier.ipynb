{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BiLSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.chdir(\"/home/yick/Projects/github.com/text-classifier\")\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### build model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "\n",
    "WORD_VECTOR_PATH = \"/home/yick/Models/tencent/embeddings/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\"\n",
    "\n",
    "def load_wv(vocab, fpath=WORD_VECTOR_PATH):\n",
    "    word2vec = {}\n",
    "    embedding_dim = None\n",
    "    with open(fpath) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            if i == 0:\n",
    "                embedding_dim = int(values[1])\n",
    "                continue\n",
    "            if len(values) != embedding_dim + 1:\n",
    "                print(f\"error values: {values[:5]}, values len: {len(values)}\")\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word2vec[word] = coefs\n",
    "    print(f\"Found {len(word2vec)} word vectors.\" )\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_mat = np.random.rand(vocab_size+1, embedding_dim)\n",
    "    word_embedding_cnt = 0\n",
    "    for word, i in vocab.items():\n",
    "        if word in word2vec:\n",
    "            word_embedding_cnt += 1\n",
    "            embedding_mat[i] = word2vec.get(word)\n",
    "    print(f\"vocab size: {vocab_size}\")\n",
    "    print(f\"word_embedding_cnt: {word_embedding_cnt}\")\n",
    "    return embedding_mat, embedding_dim\n",
    "\n",
    "\n",
    "def build_model(vocab, num_classes, max_len=30):\n",
    "    embedding_mat, embedding_dim = load_wv(vocab)\n",
    "\n",
    "    inputs = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    embeddings = Embedding(\n",
    "        input_dim=len(vocab)+1,\n",
    "\t\toutput_dim=embedding_dim,\n",
    "\t\tinput_length=max_len,\n",
    "\t\tweights=[embedding_mat],\n",
    "\t\tembeddings_regularizer=l2(0.00),\n",
    "\t\ttrainable=True\n",
    "    )(inputs)\n",
    "    print(f\"embeddings: {embeddings.shape}\")\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(embeddings)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    # x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(\n",
    "        num_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_regularizer=l2(1e-4)\n",
    "    )(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "\t\toptimizer=Adam(lr=5e-3),\n",
    "\t\tmetrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (8718, 2)\n",
      "test_df shape: (741, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "print(f\"train_df shape: {train_df.shape}\")\n",
    "print(f\"test_df shape: {test_df.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make label encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (8718, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(train_df[\"label\"].tolist())\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y = to_categorical(labels, num_classes=num_classes)\n",
    "print(f\"y shape: {y.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.409 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2390\n",
      "X shape: (8718, 30)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "texts = [\n",
    "    [w.strip() for w in jieba.cut(d.strip()) if w.strip()]\n",
    "    for d in train_df[\"text\"].tolist()\n",
    "]\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "vocab = tokenizer.word_index\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"X shape: {X.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = build_model(vocab, num_classes, max_len=30)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    mode=\"min\"\n",
    ")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    class_weight=weights,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.05,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load best model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"best_model.h5\")\n",
    "print(\"load best model done\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_texts = [[w for w in jieba.cut(d)] for d in test_df[\"text\"].tolist()]\n",
    "test_x = tokenizer.texts_to_sequences(test_texts)\n",
    "test_x = pad_sequences(test_x, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "probs = model.predict(test_x)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(preds.tolist())\n",
    "true_labels = test_df[\"label\"].tolist()\n",
    "report = classification_report(true_labels, pred_labels, digits=4)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-1.3",
   "language": "python",
   "display_name": "tf-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
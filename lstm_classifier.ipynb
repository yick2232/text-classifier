{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BiLSTM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### set random seed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set random seed done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "print(\"set random seed done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### build model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "\n",
    "# download url: https://ai.tencent.com/ailab/nlp/en/data/tencent-ailab-embedding-zh-d200-v0.2.0-s.tar.gz\n",
    "WORD_VECTOR_PATH = \"/home/yick/Models/tencent/embeddings/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\"\n",
    "\n",
    "def load_wv(vocab, fpath=WORD_VECTOR_PATH):\n",
    "    word2vec = {}\n",
    "    embedding_dim = None\n",
    "    with open(fpath) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            if i == 0:\n",
    "                embedding_dim = int(values[1])\n",
    "                continue\n",
    "            if len(values) != embedding_dim + 1:\n",
    "                print(f\"error values: {values[:5]}, values len: {len(values)}\")\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word2vec[word] = coefs\n",
    "    print(f\"Found {len(word2vec)} word vectors.\" )\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_mat = np.random.rand(vocab_size+1, embedding_dim)\n",
    "    word_embedding_cnt = 0\n",
    "    for word, i in vocab.items():\n",
    "        if word in word2vec:\n",
    "            word_embedding_cnt += 1\n",
    "            embedding_mat[i] = word2vec.get(word)\n",
    "    print(f\"vocab size: {vocab_size}\")\n",
    "    print(f\"word_embedding_cnt: {word_embedding_cnt}\")\n",
    "    return embedding_mat, embedding_dim\n",
    "\n",
    "\n",
    "def build_model(vocab, num_classes, max_len=30):\n",
    "    embedding_mat, embedding_dim = load_wv(vocab)\n",
    "\n",
    "    inputs = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    embeddings = Embedding(\n",
    "        input_dim=len(vocab)+1,\n",
    "\t\toutput_dim=embedding_dim,\n",
    "\t\tinput_length=max_len,\n",
    "\t\tweights=[embedding_mat],\n",
    "\t\ttrainable=True\n",
    "    )(inputs)\n",
    "    print(f\"embeddings: {embeddings.shape}\")\n",
    "    x = Bidirectional(\n",
    "            LSTM(128, return_sequences=True, dropout=0.5)\n",
    "    )(embeddings)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(\n",
    "        num_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_regularizer=l2(1e-4)\n",
    "    )(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "\t\toptimizer=Adam(lr=6e-3),\n",
    "\t\tmetrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (8718, 2)\n",
      "test_df shape: (741, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "data_dir = \"/home/yick/Projects/github.com/text-classifier/data\"\n",
    "train_file = os.path.join(data_dir, \"train_data.csv\")\n",
    "test_file = os.path.join(data_dir, \"test_data.csv\")\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "print(f\"train_df shape: {train_df.shape}\")\n",
    "print(f\"test_df shape: {test_df.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make label encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (8718, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(train_df[\"label\"].tolist())\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y = to_categorical(labels, num_classes=num_classes)\n",
    "print(f\"y shape: {y.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2390\n",
      "X shape: (8718, 30)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "texts = [\n",
    "    [w.strip() for w in jieba.cut(d.strip()) if w.strip()]\n",
    "    for d in train_df[\"text\"].tolist()\n",
    "]\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "vocab = tokenizer.word_index\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"X shape: {X.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n",
      "vocab size: 2390\n",
      "word_embedding_cnt: 2165\n",
      "embeddings: (?, 30, 200)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 30, 200)           478200    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 30, 256)           336896    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_22 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_23 (B (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 54)                6966      \n",
      "=================================================================\n",
      "Total params: 856,494\n",
      "Trainable params: 855,726\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Train on 8282 samples, validate on 436 samples\n",
      "Epoch 1/50\n",
      " - 13s - loss: 3.4742 - acc: 0.2004 - val_loss: 2.9680 - val_acc: 0.3257\n",
      "Epoch 2/50\n",
      " - 3s - loss: 1.9653 - acc: 0.5165 - val_loss: 1.8707 - val_acc: 0.6376\n",
      "Epoch 3/50\n",
      " - 3s - loss: 1.2493 - acc: 0.6880 - val_loss: 1.2888 - val_acc: 0.7821\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.8644 - acc: 0.7752 - val_loss: 0.8856 - val_acc: 0.8280\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.6712 - acc: 0.8285 - val_loss: 0.6745 - val_acc: 0.8349\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.5378 - acc: 0.8620 - val_loss: 0.5395 - val_acc: 0.8555\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.4731 - acc: 0.8843 - val_loss: 0.5042 - val_acc: 0.8761\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.4059 - acc: 0.8985 - val_loss: 0.4791 - val_acc: 0.8761\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3717 - acc: 0.9074 - val_loss: 0.4400 - val_acc: 0.8945\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3410 - acc: 0.9186 - val_loss: 0.3844 - val_acc: 0.9106\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3090 - acc: 0.9300 - val_loss: 0.4357 - val_acc: 0.9083\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3080 - acc: 0.9274 - val_loss: 0.4353 - val_acc: 0.8945\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.2813 - acc: 0.9355 - val_loss: 0.4391 - val_acc: 0.9014\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.2667 - acc: 0.9373 - val_loss: 0.4186 - val_acc: 0.9151\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.2578 - acc: 0.9453 - val_loss: 0.4325 - val_acc: 0.9106\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa333411c10>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = build_model(vocab, num_classes, max_len=30)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\"\n",
    ")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    class_weight=weights,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.05,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load best model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model done\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"best_model.h5\")\n",
    "print(\"load best model done\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "              上征信     0.5000    1.0000    0.6667         1\n",
      "              不舒服     0.0000    0.0000    0.0000         0\n",
      "            之前被拒了     0.0000    0.0000    0.0000         1\n",
      "             人工服务     1.0000    0.8571    0.9231         7\n",
      "             什么平台     0.8056    0.9355    0.8657        31\n",
      "          会不会放款失败     0.5000    0.2500    0.3333         4\n",
      "              利息高     0.5000    0.1667    0.2500         6\n",
      "加下微信/发信息/发个短信/发资料     0.0000    0.0000    0.0000         0\n",
      "           号码是哪来的     0.0000    0.0000    0.0000         0\n",
      "            否定/拒绝     0.8028    0.6786    0.7355        84\n",
      "          咨询APP名字     0.7143    1.0000    0.8333         5\n",
      "          咨询利息/费用     0.8684    0.8919    0.8800        37\n",
      "           咨询提前还款     1.0000    1.0000    1.0000         1\n",
      "             咨询操作     0.8947    0.5667    0.6939        30\n",
      "           咨询放款速度     0.4000    0.5000    0.4444         4\n",
      "          咨询额度-通用     0.7419    0.8519    0.7931        54\n",
      "        嗯啊哦额/模糊回答     0.7500    1.0000    0.8571         9\n",
      "       在忙/在开会/在开车     0.5455    0.6000    0.5714        10\n",
      "           已经申请到了     0.0000    0.0000    0.0000         1\n",
      "             征信不好     1.0000    1.0000    1.0000         1\n",
      "         怎么登录APP？     0.0000    0.0000    0.0000         0\n",
      "              恢复词     0.6667    0.5600    0.6087        25\n",
      "             打款渠道     0.0000    0.0000    0.0000         1\n",
      "        打过了/不要打电话     0.4545    0.8333    0.5882         6\n",
      "              打错了     0.5000    1.0000    0.6667         2\n",
      "               投诉     1.0000    0.7619    0.8649        21\n",
      "             操作繁琐     0.0000    0.0000    0.0000         0\n",
      "          无法登陆APP     0.0000    0.0000    0.0000         2\n",
      "   有信用问题/逾期记录能借款吗     0.0000    0.0000    0.0000         4\n",
      "               期数     1.0000    0.5000    0.6667         2\n",
      "           没通过怎么办     0.6364    0.4667    0.5385        30\n",
      "          犹豫中/考虑下     0.5000    0.3750    0.4286         8\n",
      "             申请条件     1.0000    0.2500    0.4000         8\n",
      "             电话号码     0.6667    0.5000    0.5714         4\n",
      "               肯定     0.8684    0.9545    0.9094       242\n",
      "             调戏AI     0.0000    0.0000    0.0000         0\n",
      "            质疑机器人     0.6667    1.0000    0.8000         4\n",
      "           运营商提示音     0.9535    0.9318    0.9425        44\n",
      "              通过率     0.0000    0.0000    0.0000         2\n",
      "               重复     0.9333    0.7368    0.8235        19\n",
      "             额度太少     0.5714    0.3636    0.4444        11\n",
      "             额度循环     0.0000    0.0000    0.0000         1\n",
      "             额度用途     0.0000    0.0000    0.0000         1\n",
      "            骗子/骗人     0.7000    0.7778    0.7368        18\n",
      "\n",
      "         accuracy                         0.7854       741\n",
      "        macro avg     0.5032    0.4843    0.4736       741\n",
      "     weighted avg     0.7987    0.7854    0.7819       741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_texts = [\n",
    "    [w.strip() for w in jieba.cut(d.strip()) if w.strip()]\n",
    "    for d in test_df[\"text\"].tolist()\n",
    "]\n",
    "test_x = tokenizer.texts_to_sequences(test_texts)\n",
    "test_x = pad_sequences(test_x, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "probs = model.predict(test_x)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(preds.tolist())\n",
    "true_labels = test_df[\"label\"].tolist()\n",
    "report = classification_report(true_labels, pred_labels, digits=4)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-1.3",
   "language": "python",
   "display_name": "tf-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
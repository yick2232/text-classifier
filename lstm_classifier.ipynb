{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BiLSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "os.chdir(\"/home/yick/Projects/github.com/text-classifier\")\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### build model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers  import Adam\n",
    "\n",
    "WORD_VECTOR_PATH = \"/home/yick/Models/tencent/embeddings/tencent-ailab-embedding-zh-d200-v0.2.0-s.txt\"\n",
    "\n",
    "def load_wv(vocab, fpath=WORD_VECTOR_PATH):\n",
    "    word2vec = {}\n",
    "    embedding_dim = None\n",
    "    with open(fpath) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            values = line.split()\n",
    "            if i == 0:\n",
    "                embedding_dim = int(values[1])\n",
    "                continue\n",
    "            if len(values) != embedding_dim + 1:\n",
    "                print(f\"error values: {values[:5]}, values len: {len(values)}\")\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word2vec[word] = coefs\n",
    "    print(f\"Found {len(word2vec)} word vectors.\" )\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_mat = np.random.rand(vocab_size+1, embedding_dim)\n",
    "    word_embedding_cnt = 0\n",
    "    for word, i in vocab.items():\n",
    "        if word in word2vec:\n",
    "            word_embedding_cnt += 1\n",
    "            embedding_mat[i] = word2vec.get(word)\n",
    "    print(f\"vocab size: {vocab_size}\")\n",
    "    print(f\"word_embedding_cnt: {word_embedding_cnt}\")\n",
    "    return embedding_mat, embedding_dim\n",
    "\n",
    "\n",
    "def build_model(vocab, num_classes, max_len=30):\n",
    "    embedding_mat, embedding_dim = load_wv(vocab)\n",
    "\n",
    "    inputs = Input(shape=(max_len,), dtype=\"int32\")\n",
    "    embeddings = Embedding(\n",
    "        input_dim=len(vocab)+1,\n",
    "\t\toutput_dim=embedding_dim,\n",
    "\t\tinput_length=max_len,\n",
    "\t\tweights=[embedding_mat],\n",
    "\t\ttrainable=True\n",
    "    )(inputs)\n",
    "    print(f\"embeddings: {embeddings.shape}\")\n",
    "    x = Bidirectional(\n",
    "            LSTM(128, return_sequences=True, dropout=0.5)\n",
    "    )(embeddings)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(\n",
    "        num_classes,\n",
    "        activation=\"softmax\",\n",
    "        kernel_regularizer=l2(1e-4)\n",
    "    )(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "\t\toptimizer=Adam(lr=5e-3),\n",
    "\t\tmetrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (8718, 2)\n",
      "test_df shape: (741, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"./data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"./data/test_data.csv\")\n",
    "print(f\"train_df shape: {train_df.shape}\")\n",
    "print(f\"test_df shape: {test_df.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make label encoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (8718, 54)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(train_df[\"label\"].tolist())\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y = to_categorical(labels, num_classes=num_classes)\n",
    "print(f\"y shape: {y.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### make vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2390\n",
      "X shape: (8718, 30)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "texts = [\n",
    "    [w.strip() for w in jieba.cut(d.strip()) if w.strip()]\n",
    "    for d in train_df[\"text\"].tolist()\n",
    "]\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "vocab = tokenizer.word_index\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"X shape: {X.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n",
      "vocab size: 2390\n",
      "word_embedding_cnt: 2165\n",
      "embeddings: (?, 30, 200)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 30, 200)           478200    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           336896    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 54)                6966      \n",
      "=================================================================\n",
      "Total params: 856,494\n",
      "Trainable params: 855,726\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "Train on 8282 samples, validate on 436 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 3.5409 - acc: 0.1855 - val_loss: 2.9025 - val_acc: 0.3280\n",
      "Epoch 2/50\n",
      " - 3s - loss: 2.0317 - acc: 0.5069 - val_loss: 1.8553 - val_acc: 0.6170\n",
      "Epoch 3/50\n",
      " - 3s - loss: 1.3219 - acc: 0.6659 - val_loss: 1.0574 - val_acc: 0.7936\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.9377 - acc: 0.7647 - val_loss: 0.7280 - val_acc: 0.8394\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.7350 - acc: 0.8120 - val_loss: 0.5949 - val_acc: 0.8486\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.5851 - acc: 0.8527 - val_loss: 0.4962 - val_acc: 0.8807\n",
      "Epoch 7/50\n",
      " - 3s - loss: 0.5012 - acc: 0.8725 - val_loss: 0.4410 - val_acc: 0.9037\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.4373 - acc: 0.8901 - val_loss: 0.4210 - val_acc: 0.8853\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.3816 - acc: 0.9026 - val_loss: 0.3487 - val_acc: 0.9128\n",
      "Epoch 10/50\n",
      " - 3s - loss: 0.3454 - acc: 0.9126 - val_loss: 0.3688 - val_acc: 0.9266\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.3158 - acc: 0.9236 - val_loss: 0.3625 - val_acc: 0.9174\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.3106 - acc: 0.9279 - val_loss: 0.4128 - val_acc: 0.9174\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fa3574ccf10>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model = build_model(vocab, num_classes, max_len=30)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    mode=\"min\"\n",
    ")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_weights_only=False,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=y,\n",
    "    class_weight=weights,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.05,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load best model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load best model done\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"best_model.h5\")\n",
    "print(\"load best model done\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "              上征信     0.2500    1.0000    0.4000         1\n",
      "              不舒服     0.0000    0.0000    0.0000         0\n",
      "            与银行区别     0.0000    0.0000    0.0000         0\n",
      "            之前被拒了     0.0000    0.0000    0.0000         1\n",
      "             人工服务     1.0000    0.5714    0.7273         7\n",
      "             什么平台     0.9062    0.9355    0.9206        31\n",
      "          会不会放款失败     0.0000    0.0000    0.0000         4\n",
      "              利息高     0.5000    0.1667    0.2500         6\n",
      "加下微信/发信息/发个短信/发资料     0.0000    0.0000    0.0000         0\n",
      "            否定/拒绝     0.8095    0.8095    0.8095        84\n",
      "          咨询APP名字     0.6667    0.8000    0.7273         5\n",
      "          咨询利息/费用     0.7895    0.8108    0.8000        37\n",
      "           咨询提前还款     1.0000    1.0000    1.0000         1\n",
      "             咨询操作     0.9000    0.6000    0.7200        30\n",
      "           咨询放款速度     0.2500    0.5000    0.3333         4\n",
      "          咨询额度-通用     0.6667    0.9259    0.7752        54\n",
      "        嗯啊哦额/模糊回答     0.8000    0.8889    0.8421         9\n",
      "       在忙/在开会/在开车     0.7500    0.6000    0.6667        10\n",
      "           已经申请到了     0.0000    0.0000    0.0000         1\n",
      "             征信不好     0.0000    0.0000    0.0000         1\n",
      "              恢复词     0.8000    0.4800    0.6000        25\n",
      "             打款渠道     0.0000    0.0000    0.0000         1\n",
      "        打过了/不要打电话     0.6667    1.0000    0.8000         6\n",
      "              打错了     0.5000    1.0000    0.6667         2\n",
      "               投诉     1.0000    0.7619    0.8649        21\n",
      "          无法登陆APP     0.2500    0.5000    0.3333         2\n",
      "   有信用问题/逾期记录能借款吗     0.0000    0.0000    0.0000         4\n",
      "              有印象     0.0000    0.0000    0.0000         0\n",
      "               期数     0.5000    0.5000    0.5000         2\n",
      "           没通过怎么办     0.9231    0.4000    0.5581        30\n",
      "          犹豫中/考虑下     0.6000    0.3750    0.4615         8\n",
      "             申请条件     0.2500    0.1250    0.1667         8\n",
      "             电话号码     0.5000    0.5000    0.5000         4\n",
      "               肯定     0.8731    0.9669    0.9176       242\n",
      "             调戏AI     0.0000    0.0000    0.0000         0\n",
      "           质疑数据安全     0.0000    0.0000    0.0000         0\n",
      "            质疑机器人     0.8000    1.0000    0.8889         4\n",
      "           运营商提示音     0.9512    0.8864    0.9176        44\n",
      "              通过率     0.0000    0.0000    0.0000         2\n",
      "               重复     0.8889    0.8421    0.8649        19\n",
      "             额度太少     1.0000    0.4545    0.6250        11\n",
      "             额度循环     0.0000    0.0000    0.0000         1\n",
      "             额度用途     0.0000    0.0000    0.0000         1\n",
      "            骗子/骗人     0.7778    0.7778    0.7778        18\n",
      "\n",
      "         accuracy                         0.7962       741\n",
      "        macro avg     0.4675    0.4586    0.4413       741\n",
      "     weighted avg     0.8108    0.7962    0.7893       741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/yick/Anaconda3/envs/tf-1.13/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_texts = [[w for w in jieba.cut(d)] for d in test_df[\"text\"].tolist()]\n",
    "test_x = tokenizer.texts_to_sequences(test_texts)\n",
    "test_x = pad_sequences(test_x, maxlen=30, padding=\"pre\", truncating=\"pre\")\n",
    "probs = model.predict(test_x)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "pred_labels = label_encoder.inverse_transform(preds.tolist())\n",
    "true_labels = test_df[\"label\"].tolist()\n",
    "report = classification_report(true_labels, pred_labels, digits=4)\n",
    "print(report)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-1.3",
   "language": "python",
   "display_name": "tf-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}